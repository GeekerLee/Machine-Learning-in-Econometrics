---
title: "Problem Set 4 -- Regression Trees and Random Forests"
author: ''
date: '`r format(Sys.Date())`'
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_chunk$set(error = TRUE)
```


## Classification trees

* Load the data set *Carseats* from the package *ISLR* and also load the package *tree* which will be used for the exercise.
* Construct a binary variable *High*, which takes on a value of Yes if the Sales variable exceeds 8, and
takes on a value of No otherwise. Hint: *ifelse()*
* Fit a classification tree to predict *High* using all other varibles in the data set. Hint: *tree()*
* Plot the fitted tree.
* Determine the "best" tree by pruning / cv and prune to this tree on some  training set. Hint: *cv.tree*
* Determine the quality of the predictions on the testing set. Hint: *predict()*

```{r, include=FALSE}
library(tree)
library(ISLR)
attach(Carseats)
High= ifelse(Sales <=8 ," No"," Yes ")
Carseats = data.frame(Carseats,High)
tree.carseats = tree(High ~ .-Sales , Carseats)
summary(tree.carseats)
plot(tree.carseats)
text(tree.carseats, pretty =0)
set.seed(2)
train = sample(1:nrow(Carseats), 200)
Carseats.test= Carseats[-train,]
High.test=High[-train]
tree.carseats = tree(High~ .-Sales ,Carseats ,subset = train)
tree.pred= predict (tree.carseats ,Carseats.test, type ="class")
table(tree.pred, High.test)
##
set.seed(3)
cv.carseats =cv.tree(tree.carseats, FUN =prune.misclass)
names(cv.carseats )
cv.carseats
par(mfrow =c(1 ,2) )
plot(cv.carseats$size,cv.carseats$dev ,type ="b")
plot(cv.carseats$k,cv.carseats$dev ,type ="b")
######################
prune.carseats =prune.misclass(tree.carseats ,best =9)
plot(prune.carseats)
text(prune.carseats, pretty =0)
######################
tree.pred= predict (prune.carseats ,Carseats.test ,type ="class")
table(tree.pred, High.test )
######################
prune.carseats=prune.misclass(tree.carseats ,best =15)
plot(prune.carseats)
text(prune.carseats, pretty =0)
tree.pred= predict (prune.carseats, Carseats.test ,type ="class")
table(tree.pred, High.test)
```

## Regression Trees
* Fit a regression tree to the Boston data we had before in the class. The depedent variable is *medv*. Hint: package *MASS*
* Do this on a training set (50% of the sample) and then evaluate the predictions on the testing set.
* Plot the tree and interpret the results!

```{r, include=FALSE}
library(MASS)
set.seed(1)
train = sample(1:nrow(Boston), nrow(Boston)/2)
tree.boston =tree(medv~.,Boston, subset =train )
summary(tree.boston)
plot(tree.boston)
text(tree.boston, pretty =0)
cv.boston =cv.tree(tree.boston)
plot(cv.boston$size,cv.boston$dev, type="b")
prune.boston = prune.tree(tree.boston ,best =5)
plot(prune.boston)
text(prune.boston, pretty =0)
yhat= predict(tree.boston, newdata = Boston[-train ,])
boston.test = Boston[-train,"medv"]
plot(yhat ,boston.test)
abline(0 ,1)
mean((yhat - boston.test)^2)
```

## Bagging and Random Forests

* Repeat the excerise from above (fitting on training set + prediction on testing set) with bagging. Hint: *randomForest* from the package with the same name.
* Finally, fit a random forest on the training data and compare the model with all previous models.

```{r, include=FALSE}
library(randomForest)
set.seed(1)
bag.boston = randomForest(medv~.,data=Boston, subset =train, mtry =13, importance = TRUE)
yhat.bag = predict (bag.boston, newdata =Boston[-train ,])
plot(yhat.bag , boston.test)
abline(0 ,1)
mean(( yhat.bag - boston.test)^2)
bag.boston = randomForest(medv~., data=Boston, subset =train ,
mtry =13, ntree =25)
yhat.bag = predict (bag.boston , newdata =Boston[-train ,])
mean((yhat.bag - boston.test)^2)
```


```{r, include=FALSE}
set.seed(1)
rf.boston = randomForest(medv~.,data =Boston , subset =train ,
mtry =6, importance = TRUE)
yhat.rf = predict(rf.boston, newdata = Boston[- train ,])
mean((yhat.rf - boston.test)^2)
importance(rf.boston)
varImpPlot(rf.boston)
```














