---
title: "Lecture 2 -- Linear Regression and Extensions"
author: "Martin Spindler"
date: '`r format(Sys.Date())`'
output:
  beamer_presentation: default
  ioslides_presentation: null
  mathjax: local
  self_contained: no
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
```

## Linear Regression
We start with a linear regression model:
$$ y_i = x_i' \beta + \varepsilon_i, i=1, \ldots,n, $$
where $x_i$ is a p-dimensional vector of regressors for observation $i$, $\beta$ a p-dimensional coefficient vector, and $\varepsilon_i$ iid error terms with $\mathbb{E} [\varepsilon_i|x_i]= 0$.

The ordinary least squares (ols) estimator for $\beta$ is defined as

$$ \hat{\beta} = \arg\min_{\beta \in \mathbb{R}^p} \sum_{i=1}^n (y_i - x_i' \beta)^2.$$


## Linear Regression
If the Gram matrix $\sum_{i=1}^n x_i x_i'$ is of full rank, the ols estimate is given by 

$$ \hat{\beta} = (\sum_{i=1}^n x_i x_i')^{-1} (\sum_{i=1}^n x_i y_i). $$

The residuals $\hat{\varepsilon}_i$ are defined as
$$ \hat{\varepsilon}_i = y_i - x_i'\hat{\beta}. $$
For an observation $x$ the *fitted* or *predicted* values are given by 
$$ \hat{y} =  x'\hat{\beta}. $$

## Linear Regression

In matrix notation we can write

$$ Y=X  \beta + \varepsilon $$

with $Y=\begin{pmatrix} y_1 & \ldots & y_n \end{pmatrix}$, $\varepsilon=\begin{pmatrix} \varepsilon_1 & \ldots & \varepsilon_n \end{pmatrix}$ and $X$ is a $n \times p$-matrix with observation $i$ forming the $i$th row of the matrix $X$.

The ols estimate $\hat{\beta}$ can then be written as

$$ \hat{\beta} = (X'X)^{-1}X'y.$$

## Linear Regression

Under homoscedastic errors, i.e. $\mathbb{V} \varepsilon_i=\sigma^2$, we have that

$$ \mathbb{V}(\hat{\beta})= (X'X)^{-1} \sigma^2.$$

Asymptotically, the ols estimate is normal distributed:

$$ \hat{\beta} \sim N(\beta, (X'X)^{-1} \sigma^2).$$

This can be used for testing hypotheses and construction of confidence intervals.

## Linear Regression
$$ z_j=\frac{\hat{\beta_j}}{\hat{\sigma}^2 \sqrt{v_j}}$$
where $v_j$ is the $j$th diagonal element of $(X'X)^{-1}$.  
Under the null hypothesis $\beta_j=0$ the *Z-score* / *t-statistic* $z_j$ is $t_{n-p-1}$-distributed.

## Linear Regression

Remark: In the high-dimensional-setting, i.e. $p >> n$ the Gram Matrix is rank deficient and the ols estimate is not uniquely defined and the variance of the parameter estimate is unbounded.

## Extensions
- Polynomial Regression
- Step Functions
- Basis Functions
- Regression Splines
- Smoothing Splines

## Extensions | Remarks
* Although the linear regression model looks quite simple, it can be extended / modified to model complex relations.
* For the extensions we consider without loss of generality univariate regressions:
  
  $$ y_i=\beta_0 + \beta_1 x_i + \varepsilon_i$$

## Extensions | Polynomial Regression

To make the linear specification more flexible, we might include higher-order polynomials:
  
  $$y_i = \beta_0 + \beta_1 x_i + \beta_2 x_i^2 + \ldots \beta_p x_i^p + \varepsilon_i$$

  
* Estimation by ols
* Quite flexible, but usually p=3 or p=4
* Higher order polynomials (p > 5) might lead to strange fits (overfitting), especially at the boundary.

## Extensions | Polynomial Regression - Example
```{r, include=FALSE}
library(ISLR)
attach(Wage)
# Fourth orger polynomial
fit4 = lm(wage ~ poly(age, 4, raw=T), data=Wage)
# fit = lm(wage ~ age + I(age^2) + I(age^3) + I(age^4), data=Wage)
agelims = range(age)
age.grid = seq(from=agelims[1], to=agelims[2])
preds4 = predict(fit4, newdata=list(age=age.grid), se=TRUE)
se.bands4 = cbind(preds4$fit + 2*preds4$se.fit, preds4$fit - 2*preds4$se.fit) #95% confidence intervall
```


```{r, include=FALSE}
# calculation higher order polynomials
fit3 = lm(wage ~ poly(age, 3, raw=T), data=Wage)
preds3 = predict(fit3, newdata=list(age=age.grid), se=TRUE)
fit5 = lm(wage ~ poly(age, 5, raw=T), data=Wage)
preds5 = predict(fit5, newdata=list(age=age.grid), se=TRUE)
```

```{r}
par(mfrow=c(1,2), mar=c(4.5,4.5,1,1), oma=c(0,0,4,0))
plot(age, wage, xlim=agelims, cex=.5, col="darkgrey")
title("Polynomial of order 4", outer=F)
lines(age.grid, preds4$fit, lwd=2, col="blue")
matlines(age.grid, se.bands4, lwd=1, col="blue", lty=3)
plot(age, wage, xlim=agelims, cex=.5, col="darkgrey")
lines(age.grid, preds4$fit, lwd=2, col="blue")
lines(age.grid, preds5$fit, lwd=2, col="red")
lines(age.grid, preds3$fit, lwd=2, col="green")
title("Different higher order polynomials")
legend("bottomright", c("Third order", "Fourth order", "Fifth order"), 
       lty=c(1,1,1), lwd=rep(2,3),col=c("green","blue", "red"), cex=0.65)
```


## Extensions | Step Functions

* Definition: Step functions are functions which are constant on each part of a partition of the domain.
* Univariate Regression: choosing $K$ cut points $c_1, \ldots, c_K$ and defining new auxiliary variables:  
  $C_0(x)= 1(x < c_1)$, $C_1(x)=1(c_1 \leq x < c_2)$, $\ldots$, $C_K(x)=1(c_K \leq x)$
  * $1(\cdot)$ is the so-called indicator function which is $1$ is the condition is true and $0$ otherwise.
* This gives us the following regression:
  $$ y_i = \beta_0 = \beta_1 *C_1(x_i) + \ldots + \beta_K *C_K(x_i) + \varepsilon_i $$
  
  ## Extensions | Step Functions
  
  * Note: $C_0(x) + \ldots + C_K(x) = 1$ and hence we drop $C_0=(\cdot)$ to avoid multicollinearity.
* Interpretation $\beta_0$
  * Example: wage regression

## Extensions | Step Functions
```{r, include=FALSE}
Wage$agegroup<-cut(Wage$age, c(18,25,35,45,55,65,80))
fitgroup = lm(wage ~ agegroup, data=Wage)
predsgroup = predict(fitgroup, newdata=list(agegroup=cut(18:80, c(18,25,35,45,55,65,80))), se=TRUE)
se.bandsgr = cbind(predsgroup$fit + 2*predsgroup$se.fit, predsgroup$fit - 2*predsgroup$se.fit) #95% confidence intervall
```

```{r}
plot(age, wage, xlim=agelims, cex=.5, col="darkgrey")
title("Regression with step functions", outer=T)
lines(18:80, predsgroup$fit, lwd=2, col="blue")
matlines(age.grid, se.bandsgr, lwd=1, col="blue", lty=3)
```

## Extension | Basis Functions

* Idea: family of functions or transformations that can be applied to a variable: $b_1(x), \ldots, b_K(x)$ (basis functions)
* Regression: $$y_i = \beta_0 + \beta_1 b_1(x_i) + \ldots + \beta_K b_K(x_i) + \varepsilon_i $$
* Examples
+ Polynomial regression: $b_j(x_i)=x^j_i$
+ Piecewise constant functions (step functions): $b_j(x_i)=1(c_j \leq x_i < c_{j+1})$
+ Regressions splines (coming next)